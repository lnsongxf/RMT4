

\input grafinp3
%\input grafinput8
\input psfig
%\eqnotracetrue

\showchaptIDtrue
\def\@chaptID{2.}

%\eqnotracetrue

\hbox{}

\def\toone{{t+1}}
\def\ttwo{{t+2}}
\def\tthree{{t+3}}
\def\Tone{{T+1}}
\def\TTT{{T-1}}
\def\rtr{{\rm tr}}
\footnum=0
\chapter{Dynamic Programming\label{dynamicdp1}}%
This chapter introduces basic ideas and methods of \idx{dynamic programming}.%
\index{dynamic optimization}%
\NFootnote{This chapter aims to the reader
to start using the methods quickly.  We hope to  promote demand
for further  and more rigorous study of the
subject. In particular see Bertsekas (1976),
Bertsekas and Shreve (1978), Stokey and Lucas (with Prescott) (1989),
Bellman (1957), and Chow (1981). This chapter covers
much of the same material as Sargent (1987b, chapter 1).}
It sets out the basic elements of a recursive optimization
problem,  describes  a key functional equation called the Bellman equation,
 \index{Bellman equation}%
\index{value function!|see{Bellman equation}}%
presents three methods for
solving the Bellman equation, and gives the Benveniste-Scheinkman
formula for the derivative of the optimal value function.   Let's dive in.
\auth{Bertsekas, Dimitri}  \auth{Shreve, Steven} \auth{Lucas, Robert E., Jr.}
\auth{Prescott, Edward C.} \auth{Stokey, Nancy L.}
\auth{Bellman, Richard} \auth{Chow, Gregory}

\section{Sequential problems}

  Let $\beta \in (0,1)$ be a discount
factor. %^\<sequential: recursive>
\index{sequential!|see{recursive}}
  We want to choose an infinite
sequence of ``controls'' $\{u_t\}_{t=0}^\infty$ to maximize
$$ \sum_{t=0}^\infty \be^t r(x_t,u_t),\EQN 129-a$$
subject to $x_{\toone} =g(x_t,u_t)$, with $x_0\in \bbR^n$ given.
We assume that $r(x_t,u_t)$ is a concave function and that the set
$\{(x_{t+1}, x_t): x_{t+1} \le g(x_t,u_t), u_t \allowbreak \in
\bbR^k\}$ is convex and compact. Dynamic programming seeks a
time-invariant {\it policy function\/} $h$ mapping
\index{policy function}%
\index{state}%
the {\it state\/} $x_t$ into the control $u_t$, such that
the sequence $\{u_s\}_{s=0}^\infty$ generated by iterating the
two functions
$$\eqalign{ u_t & = h(x_t) \cr
            x_{t+1} &= g(x_t,u_t), \cr} \EQN new1 $$
starting from initial condition $x_0$ at $t=0$, solves the original problem.
 A solution in the form of equations \Ep{new1} is said
to be {\it recursive}.
To find the policy function $h$ we need to know another function
$V(x)$ that expresses the optimal value of the original problem, starting
from an arbitrary initial condition $x\in X$.
This is called the {\it value function}. In particular,  \index{value function}
define
$$V(x_0)=\max_{\{u_s\}_{s=0}^\infty} \sum_{t=0}^\infty \be^t
r(x_t,u_t),\EQN 129-b$$
where again the maximization is subject to $x_{\toone} =g(x_t,u_t)$,
with $x_0$ given.   Of course,  we cannot possibly
expect to know $V(x_0)$ until after we have solved
the problem, but let's proceed on faith. If we knew $V(x_0)$, then
 the policy function $h$ could be computed by
solving for each $x \in X$ the problem
$$\max_u \{r(x,u)+\be V(\tilde x)\},\EQN 128-a$$
where the maximization is subject to $\tilde x=g(x,u)$ with $x$ given, and
$\tilde x$ denotes the state next period.
Thus, we have exchanged the original
 problem of finding an infinite {\it sequence\/}
of controls that maximizes expression \Ep{129-a} for the problem
of finding the optimal value function $V(x)$ and a function $h$ that solves
the continuum of maximum problems \Ep{128-a}---one maximum
problem for each value of $x$.
This exchange doesn't look like progress, but
we shall see that it often is.



Our task has become jointly to solve for $V(x),h(x)$, which
are linked by the {\it Bellman equation}
$$V(x) =\max_u \{r(x,u)+\be V[g(x,u)]\}. \EQN 128-b$$
The maximizer of the right side of equation \Ep{128-b} is  a
{\it policy function} $h(x)$ that satisfies
$$V(x)=r[x,h(x)]+\beta V\{g[x,h(x)]\}.\EQN 128-c$$
Equation \Ep{128-b} or
\Ep{128-c} is a {\it functional equation} to be solved for the
pair of unknown functions $V(x), h(x)$.
\index{functional equation}

Methods for solving the Bellman equation  are based on
mathematical structures that vary in their details depending on
the precise nature of the functions $r$
 and $g$.\NFootnote{There are alternative sets of
conditions that make  the maximization \Ep{128-a} well behaved.
  One set of conditions is as follows: (1) $r$ is
concave and bounded, and
 (2) the constraint set generated by $g$ is convex and
compact, that is, the set of $\{(x_{\toone},x_t): x_{\toone}\le g(x_t,u_t)\}$
for admissible $u_t$ is convex and compact.  See Stokey, Lucas, and
Prescott  (1989) and Bertsekas
(1976) for further details of convergence results.  See Benveniste and
Scheinkman (1979) and Stokey, Lucas, and Prescott (1989)
 for the results on differentiability of
the value function.
  In Appendix \use{functional} (see Technical
 Appendixes),
 we describe the mathematics
for one standard set of assumptions about $(r,g)$. In chapter \use{dplinear},
we describe it for another set of assumptions about
$(r,g)$.}
All of these structures contain versions of the following four
findings.
  Under
various particular assumptions about
$r$ and $g$,
\auth{Lucas, Robert E., Jr.}\auth{Stokey, Nancy L.}%
\auth{Prescott, Edward C.}%
\auth{Bertsekas, Dimitri}%
\auth{Benveniste, Lawrence}\auth{Scheinkman, Jose}%
it turns out that
\medskip
\item{1.} The functional equation
\Ep{128-b}
has a unique strictly concave solution.
\medskip
\item{2.} This solution is
approached in the limit as $j\to\infty$ by iterations on
$$V_{j+1}(x) =\max_u \{r(x,u)+\be V_j(\tilde x)\},   %% LAYOUT_TRICK (leaving out) \EQN 127-a
$$
 subject to $\tilde x=g(x,u), x$ given,
starting from any bounded and continuous initial $V_0$.
\medskip
\item{3.} There is a unique and time-invariant optimal policy of the form
$u_t=h(x_t)$, where $h$ is chosen to maximize the right side of
\Ep{128-b}.%\NFootnote{The  time invariance of the  policy function
%$u_t=h(x_t)$ is very convenient
%econometrically, because we can impose a single decision rule
%for all periods. This lets us pool data across periods to estimate
%the free parameters of the  return and transition functions that
%underlie the decision rule.}
%%% LAYOUT TRICK -- dropped footnote for reasons of space, not science.

\medskip
 \item{4.} Off
corners, the limiting value function $V$ is differentiable.

\medskip
\medskip
Since the value function is differentiable, the first-order
necessary condition for problem \Ep{128-a} becomes\NFootnote{Here
and below, subscript $1$ denotes the vector of  derivatives with
respect to the $x$ components and subscript $2$ denotes the
derivatives with respect to the $u$ components.}
$$r_2\!(x,u)+\be V'\!\{g(x,u)\} \, g_2\!(x,u) = 0.   \EQN 130new-a$$
If we also assume that the policy function $h(x)$ is
differentiable, differentiation of expression \Ep{128-c}
yields\NFootnote{Benveniste and Scheinkman (1979) proved
differentiability of $V(x)$ under broad conditions that do not require that  $h(x)$ be differentiable.
For conditions under which $h(x)$ is  differentiable, see
 Santos (1991,1993).}\auth{Santos, Manuel S.}%
%Hence, note that the derivative of the value function in
%expression \Ep{130new-c} holds more generally than suggested by
%our calculations.}
$$\EQNalign{
V'\!(x)= &\; r_1\![x,h(x)]+r_2\![x,h(x)]\,h'(x)    \cr
\noalign{\vskip.1cm}
       &+ \beta V'\!\left\{g[x,h(x)]\right\}\,\Bigl\{g_1\![x,h(x)]
              + g_2\![x,h(x)]\,h'(x) \Bigr\}.    \EQN 130new-b \cr}
$$
When the states and controls can be defined in such a way that
only $u$ appears in the transition equation, i.e., $\tilde x = g(u)$:
the derivative of the value function becomes, after substituting
expression \Ep{130new-a} with $u=h(x)$ into \Ep{130new-b},
$$ V'\!(x)= r_1\![x,h(x)].      \EQN 130new-c$$
This is a version of a formula of Benveniste and Scheinkman (1979).
\index{Benveniste and Scheinkman formula}%




% with
%$$V'(x) ={\part r\over\part x} [x,h(x)] +\be {\part g\over\part x} [x,h(x)]
%  V' \{g[x,h(x)]\}.\EQN 130$$
%This is a version of a formula of Benveniste and Scheinkman (1979).
%   We  often encounter  settings in
%which the transition law can be formulated
%so that the  state $x$ does not appear in it,
%so that ${\partial g \over \partial x} = 0$, which makes
%equation \Ep{130} become
%$$V'(x) ={\part r\over\part x} [x,h(x)].    \EQN 1300$$

\medskip


 % In appendix on functional analysis,
 %we briefly describe the mathematics
%for one standard set of assumptions about $r,g$. Later
%we will describe it for another set of assumptions about
%$(r,g)$.
At this point, we describe three broad computational
strategies that apply in various contexts.

\subsection{Three computational methods}

There are three main  types of
computational methods for solving dynamic programs.
All aim to solve the functional equation
\Ep{128-a}.
\medskip
\noindent{\bf Value function iteration.}
\index{value function!iteration} The first method proceeds by constructing
a sequence of value functions and associated policy
functions.  The sequence is created by iterating on the following
equation, starting from $V_0=0$, and continuing until $V_j$ has
converged:
%\NFootnote{See  Appendix \use{functional} on functional analysis (see Technical
% Appendixes) for
%what it means for a sequence of functions
%to converge.}
$$V_{j+1}(x) =\max_u \{r(x,u)+\be V_j(\tilde x)\},\EQN new2$$
subject to $\tilde x=g(x,u), x$ given.\NFootnote{See  Appendix \use{functional}
on functional analysis (see Technical Appendixes) for
what it means for a sequence of functions
to converge. A proof
of the uniform convergence of iterations on equation
\Ep{new2} is contained in that appendix.}
%\NFootnote{A proof
% of the uniform convergence of iterations on equation
%\Ep{new2} is contained in Appendix \use{functional} on functional
%analysis (see Technical
% Appendixes).}
This method is called {\it value function iteration\/} or
{\it iterating on the Bellman equation}.

%\beginleftbox Tom, this (1.27) is the same as EQN 127-a. \endleftbox
\medskip
\noindent{\bf Guess and verify.} \index{guess-and-verify method}
 A second method involves guessing and verifying a solution $V$
to equation \Ep{128-b}.  This method relies on the uniqueness
of the solution to the equation, but because it relies on luck
in making a good guess, it is not generally available.
\medskip
\noindent{\bf Howard's improvement algorithm.}
\index{policy improvement algorithm}%
A third method, known as {\it policy function iteration} or {\it Howard's
improvement algorithm},
consists of the following steps:
\medskip
\item{1.} Pick a feasible policy, $u = h_0(x)$, and compute
the value associated with operating forever with that policy:
$$ V_{h_j}(x) = \sum_{t=0}^\infty \beta^t r[x_t, h_j(x_t)]  ,$$
where $x_{t+1} = g[x_t, h_j(x_t)]$, with $j=0$.

\item{2.}  Generate a new policy $u=h_{j+1}(x)$ that solves the
two-period problem
$$ \max_u \{r(x,u) + \beta V_{h_j}[g(x,u)]\}  ,$$
for each $x$.
\vskip2cm  %%LAYOUT_TRICK
\item{3.}  Iterate over $j$ to convergence on steps 1 and 2.
\medskip

In  Appendix \use{functional} (see Technical
 Appendixes),
 we describe some conditions under which the policy improvement algorithm
converges to the solution of the Bellman equation.  The policy improvement algorithm often
converges faster than does value function iteration (e.g., see
exercise {\it \the\chapternum.1\/} at the end of this chapter).\NFootnote{The speed of
the policy improvement  algorithm comes from  its implementing Newton's method, which converges quadratically
while iteration on the Bellman equation converges at a linear rate.
See chapter \use{practical} and    Appendix \use{functional}  (see Technical
 Appendixes).}
The policy improvement
algorithm is also a building block for methods used to study
government policy in chapter \use{credible}.


     Each of our three  methods for solving dynamic programming problems has its uses. Each is easier said than done,
      because it is typically impossible
analytically to compute even {\it one\/} iteration on equation \Ep{new2}.
This fact thrusts us into the domain of computational methods for
approximating solutions: pencil and paper are insufficient.
Chapter \use{practical} describes  computational methods that
can applied to  problems that cannot be solved by hand.
Here  we shall describe the first of two special
types of problems for which analytical solutions {\it can\/} be
obtained.  It involves
 Cobb-Douglas constraints and logarithmic preferences.
Later, in chapter \use{dplinear}, we shall describe a specification
with linear constraints and quadratic preferences.
For that special case, many analytic results are available.
These two classes have been important in economics as sources of
examples and as inspirations for approximations.
\index{logarithmic preferences}
\subsection{Cobb-Douglas transition, logarithmic preferences}
Brock and Mirman (1972) used the following optimal growth
example.\NFootnote{See also Levhari and Srinivasan (1969).}
A planner chooses sequences $\{c_t,k_{t+1}\}_{t=0}^\infty$
to maximize
$$ \sum_{t=0}^\infty \beta^t \ln(c_t)$$
subject to a given value for $k_0$ and a transition law
$$ k_{t+1} + c_t = A k_t^\alpha ,\EQN brock2$$
where  $A >0, \alpha \in (0,1), \beta \in (0,1)$.
\index{optimal growth}

  This problem can be solved ``by hand,'' using any of our three methods.
We begin with iteration on the Bellman equation.
Start with $v_0(k) = 0$, and solve the one-period problem:
choose $c$ to maximize $\ln (c)$ subject to
$c + \tilde k = A k^\alpha.$  The solution
is evidently to set $c=Ak^\alpha, \tilde k=0$, which
produces an optimized value $v_1(k) = \ln A + \alpha \ln k$.
% To compute $v_1(k)$, we solve the problem
%of maximizing $\ln (c) + \beta [\ln A + \alpha \ln \tilde k]$,
%subject to $\tilde k + c = A k^\alpha$, which results in
%$c = A k^\alpha, \tilde k = 0, v_1(k) = \ln A + \alpha \ln k$.
At the second step, we find $c= {1 \over 1 + \beta \alpha} A k^\alpha,
\tilde k = {\beta \alpha \over 1 + \beta \alpha}A k^\alpha,
v_2(k) = \ln {A \over 1 + \alpha \beta} + \beta \ln A
+ \alpha \beta \ln{\alpha \beta A \over 1 + \alpha \beta}
+ \alpha (1+\alpha \beta) \ln k$.  Continuing, and using
the algebra of geometric series, gives the
limiting policy functions $c=(1-\beta \alpha) A k^\alpha,
\tilde k = \beta \alpha  A k^\alpha$,
and the value function $v(k) = (1-\beta)^{-1}\{ \ln[A(1-\beta\alpha)]
+ {\beta \alpha \over 1-\beta \alpha} \ln(A\beta \alpha)\}
+ {\alpha \over 1-\beta\alpha} \ln k$.
\smallskip

  Here is how the \idx{guess-and-verify method} applies to
this problem.  Since we already know the answer, we'll guess
a function of the correct form, but leave its coefficients
undetermined.\NFootnote{This is called the {\it method of undetermined
coefficients\/}.}  \index{method of undetermined coefficients}
Thus, we make the guess
$$v(k) =E+F\ln k,\EQN 136$$
where $E$ and $F$ are undetermined constants.
The left and right sides of equation \Ep{136} must agree for all values
of $k$.
For this guess, the first-order necessary condition for the
maximum problem on the right side of  equation \Ep{new2} implies the
following formula for the optimal policy $\tilde k=h(k)$, where $\tilde k$ is
next period's value and $k$ is this period's value of the capital stock:
$$\tilde k={\be F\over 1+\be F} \ Ak^\a.\EQN 137$$
%%%%%
%Substituting \Ep{137} into the right side of {\bf this
%argument must be patched up because the order has
%been changed from the red book!  What was here was a formula
%for the derivative.}
% gives
%$$v'(k)=(1+\be F)\a k^{-1}.\EQN 138$$
%Differentiating \Ep{136} gives
%$$v'(k)=Fk^{-1}.\EQN 139$$
%Equating \Ep{138} and \Ep{139} permits one to solve for $F$, $F=\a/(1-\a\be)$.
%Substituting this expression for $F$ back into \Ep{136} and \Ep{137} gives
%$$\eqalign{ v(k)&= E+{\a\over 1-\a\be}\ \ln k\cr
%\tilde k&=A\be\a k^\a.\cr}\EQN 140$$
%The fact that expressions \Ep{138} and \Ep{139} for $v'(k)$ have identical
%functional forms both verifies the original guess \Ep{136} and permits one to
%solve for the undetermined coefficient $F$.  An alternative procedure for
%verifying the guess involves
%%%%%
Substitute equation \Ep{137} into the  Bellman
equation and equate the result to the right side of  equation
\Ep{136}.  Solving the
resulting equation for $E$ and $F$ gives $F=\a/(1-\a\be)$ and
$E=(1-\be)^{-1} [ \ln A(1-\a\be)+ {\be\a\over 1-\a\be}\ \ln
 A\be\a ].$
%In Exercise 1.1, the reader is asked to construct the same solution \Ep{137}
%to
%the functional equation, using the method of iterating on Bellman's equation
%(1.26) starting from $v_0(k)=0$.
It follows that
$$\tilde k=\be\a A k^\a.\EQN 140$$
Note that
the term $F=\a/(1-\a\be)$  can be interpreted as a
geometric sum $\a[1+\a\be+(\a\be)^2+\ldots]$.

Equation \Ep{140} shows that the optimal policy is to have capital move
according to the difference equation $k_{\toone}=A\be\a k_t^\a$, or
 $\ln k_{\toone}=\ln A\be\a+
\a \ln k_t$.  That $\a$ is less than $1$
 implies that $k_t$ converges as $t$ approaches infinity for
any positive initial value $k_0$.  The stationary point is given by the
solution of $k_\infty =A\be\a k_\infty^\a$, or $k_\infty^{\a-1}=(A\be\a)^{-1}$.

\subsection{Euler equations}
In many problems, there is no unique way of defining states and controls, and
several alternative definitions lead to the same solution of the problem.
%Sometimes the states and controls can be defined in such a way that $x_t$ does
%not appear in the transition equation, so that $\part g_t/\part x_t\equiv 0$.
%In this case,
%the first-order condition for the problem on the right
%side of the \idx{Bellman equation} in conjunction with the
%\idx{Benveniste-Scheinkman formula} implies
%$${\part r_t\over\part u_t} (x_t,u_t) +{\part g_t\over\part u_t} (u_t) \cdot
%{\part r_{\toone} (x_{\toone},u_{\toone})\over\part x_{\toone}} =0,\qquad x_{\toone}
%=g_t(u_t).$$
When the states and controls can be defined in such a way that
only $u$ appears in the transition equation, i.e., $\tilde x = g(u)$:
the first-order condition for the problem on the right
side of the \idx{Bellman equation} (expression \Ep{130new-a})
in conjunction with the \idx{Benveniste-Scheinkman formula}
(expression \Ep{130new-c}) implies
$$r_2\!(x_t,u_t)+\be \, r_1\!(x_{t+1},u_{t+1}) \, g'\!(u_t) =0, \qquad x_{\toone}
=g(u_t).$$
The first equation is called an {\it Euler equation}.
  Under
circumstances in which the second equation can be inverted to yield $u_t$ as a
function of $x_{\toone}$, using the second equation to eliminate $u_t$ from the
first equation produces a second-order difference equation in $x_t$, since
eliminating $u_{t+1}$ brings in $x_{t+2}$.


\subsection{A sample Euler equation}
As an example of an Euler equation,
 consider the Ramsey problem of choosing
$\{c_t, k_{t+1}\}_{t=0}^\infty$ to  maximize
$\sum_{t=0}^\infty \beta^t u(c_t)$ subject to
$c_t + k_{t+1} = f(k_t)$, where $k_0$ is given and
the one-period utility function satisfies
$u'(c) >0, u''(c) < 0, \lim_{c_t \searrow 0}$
$u'(c_t) = \infty$, and where $f'(k) >0, f''(k) <0$.
Let the state be $k$ and the control be $\tilde k$, where $\tilde k$ denotes
next period's value of $k$.  Substitute $c= f(k) -\tilde k$
into the utility function and express the Bellman equation
as
$$ v(k) = \max_{\tilde k} \biggl\{u[f(k) - \tilde k] + \beta v(\tilde k)\biggr\}. \EQN ramsey2$$
Application of the    Benveniste-Scheinkman formula
gives
$$v'(k) = u'[f(k) - \tilde k] f'(k). \EQN benev2$$
Notice that the first-order condition for the maximum problem
on the right side of equation  \Ep{ramsey2} is
$ - u'[f(k)-\tilde k] + \beta v'(\tilde k) =0$, which, using equation
\Ep{benev2}, gives
$$ u'[f(k) - \tilde k] = \beta u'[f(\tilde k)- \hat k] f'(\tilde k), \EQN ramsey3 $$
\eject  %LAYOUT_TRICK
\noindent     %LAYOUT_TRICK
where $\hat k$ denotes the two-period-ahead value of $k$.
Equation \Ep{ramsey3} can be expressed
as
$$ 1 = \beta {u'(c_{t+1})\over u'(c_t)} f'(k_{t+1}), $$
an Euler equation  that is exploited extensively in the theories
of finance, growth, and real business cycles.



\section{Stochastic control problems}
 We now consider a modification of problem \Ep{129-a} to permit uncertainty.
Essentially, we add some well-placed shocks to the previous
nonstochastic problem. So long as the shocks are either
independently and identically distributed or Markov,  straightforward
modifications of the  method for handling the nonstochastic problem
will work.


Thus, we modify the transition equation and consider
the problem of maximizing
$$E_0 \sum_{t=0}^\infty \be^t r(x_t,u_t),\qquad 0<\be<1,\EQN 150$$
subject to
$$x_{\toone} =g(x_t,u_t,\eps_{\toone}),\EQN 151$$
with
$x_0$ known and given at $t=0$, where $\eps_t$ is a sequence of independently
and identically distributed random variables with cumulative probability
distribution function prob$\{\eps_t\le e\} =F(e)$ for all $t$; $E_t(y)$ denotes
the mathematical expectation of a random variable $y$, given information known
at $t$.  At time $t$, $x_t$ is assumed to be known, but $x_{t+j}, j\ge 1$ is
not known at $t$.  That is, $\eps_{t+1}$ is realized at $(t+1)$, after $u_t$
has been chosen at $t$.  In problem \Ep{150}--\Ep{151}, uncertainty is
injected by
assuming that $x_t$ follows a random difference equation.

Problem \Ep{150}--\Ep{151} continues to have a recursive structure,
stemming
jointly from the additive separability of the objective function \Ep{150} in
pairs  $(x_t,u_t)$ and from the difference equation characterization of the
transition law \Ep{151}.  In particular, controls dated $t$ affect returns
$r(x_s,u_s)$ for $s\ge t$ but not earlier.  This feature implies that dynamic
programming methods remain appropriate.

The problem is to maximize expression
\Ep{150} subject to equation \Ep{151} by choice of a
``policy''
or ``contingency plan'' $u_t=h(x_t)$.  The  Bellman
equation \Ep{128-b} %%Tom, another original 1.28
becomes
$$V(x)=\max_u \{r(x,u) +\be E[V[g(x,u,\eps)]|x]\},\EQN 152$$
where $E\{V[g(x,u,\eps)]|x\}=\int V[g(x,u,\eps)]dF(\eps)$ and where $V(x)$ is
the optimal value of the problem starting from $x$ at $t=0$.  The solution
$V(x)$ of equation \Ep{152} can be computed by iterating on
$$V_{j+1} (x)=\max_u \{r(x,u)+\be E[V_j[g(x,u,\eps)]|x]\},\EQN 153$$
starting from any bounded continuous initial $V_0$.  Under various
particular regularity conditions, there obtain versions of the same
four properties listed earlier.\NFootnote{See
 Stokey and Lucas (with Prescott)
(1989),
or the framework presented in  Appendix \use{functional} (see Technical
Appendixes).}

\auth{Lucas, Robert E., Jr.}  \auth{Prescott, Edward C.}  \auth{Stokey, Nancy L.}

The first-order necessary condition for the problem on the right
side of equation \Ep{152} is
$$r_2\!(x,u)+\be \, E \!\left\{ V'\![g(x,u,\eps)] \, g_2\!(x,u,\eps) \Big| x\right\} =0,$$
% $${\part r(x,u)\over\part u} +\be E \left[ {\part g\over\part u} (x,u,\eps)
% V'[g(x,u,\eps)]|x\right] =0,$$
which we obtained simply by differentiating the right side of equation
\Ep{152}, passing
the differentiation operation under the $E$ (an integration) operator.  Off
corners, the value function satisfies
$$\EQNalign{
V'\!(x)= &\; r_1\![x,h(x)]+r_2\![x,h(x)]\,h'(x)    \cr
\noalign{\vskip.1cm}
       &+\be \, E\!\left\{ V'\!\{g[x,h(x),\eps]\}\,\{g_1\![x,h(x),\eps]
              + g_2\![x,h(x),\eps]\,h'(x) \} \Big|  x\right\}. \cr}$$
% $$V'(x) ={\part r\over\part x} [x,h(x)] +\be \, E\!\left\{ {\part g\over\part x}
% [x,h(x),\eps] V'(g[x,h(x),\eps])|x\right\}.$$
When the states and controls can be defined in such a way that
$x$ does not appear in the transition equation, the formula for $V'(x)$
becomes
$$ V'\!(x)= r_1\![x,h(x)].$$
%In the special case in which $\part g/\part x\equiv 0$, the formula for $V'(x)$
%becomes
%$$V'(x)={\part r\over\part x} [x,h(x)].$$
Substituting this formula into the first-order necessary condition for the
problem gives the stochastic Euler equation
$$r_2\!(x,u)+\be E \left[ r_1\!(\tilde x,\tilde u) \, g_2\!(x,u,\eps) \Big| x\right] =0,$$
% $${\part r\over\part u} (x,u)+\be E\left[ {\part g\over\part u} (x,u,\eps)
% {\part r\over\part x} (\tilde x,\tilde u)|x\right]=0,$$
where tildes over $x$ and $u$ denote next-period values.




\section{Concluding remarks}
This chapter has put forward basic tools and findings: the Bellman
equation and several approaches to solving it; the Euler equation;
and the Benveniste-Scheinkman formula.
To appreciate and believe in  the power of these tools requires more
words and
more  practice than we have yet supplied.
In the next several chapters, we put the basic tools to work
in different contexts  with particular specification
of return  and transition equations designed to render
the Bellman equation susceptible to further analysis and computation.



%\section{Exercise}

\showchaptIDfalse
\showsectIDfalse
\section{Exercise}
\showchaptIDtrue
\showsectIDtrue
\medskip\noindent
{\it Exercise \the\chapternum.1}  \quad {\bf Howard's policy iteration algorithm }
\medskip\noindent
Consider the Brock-Mirman problem: to maximize
$$E_0\sum_{t=0}^\infty \beta^t \ln c_t,$$
subject to $c_t+ k_{t+1} \leq Ak_t^\alpha\theta_t$, $k_0$ given,
 $A>0$, $1>\alpha>0$, where
$\{\theta_t\}$ is an i.i.d.\ sequence with $\ln\theta_t$
distributed according to a
normal distribution with mean zero and variance $\sigma^2$.

Consider the following algorithm.  Guess at a policy of the form
$k_{t+1}=h_0(Ak_t^\alpha \theta_t)$ for any constant $h_0\in(0,1)$.  Then form
$$J_0(k_0,\theta_0) =E_0\sum_{t=0}^\infty \beta^t \ln (Ak_t^\alpha \theta_t-h_0
 Ak_t^\alpha
\theta_t).$$
Next choose a new policy $h_1$ by maximizing
$$\ln (Ak^\alpha \theta-k') +\beta E J_0(k',\theta'),$$
where $k'=h_1Ak^\alpha\theta$.  Then form
$$J_1(k_0,\theta_0) =E_0\sum_{t=0}^\infty \beta^t \ln (Ak_t^\alpha \theta_t
-h_1Ak_t^\alpha
\theta_t).$$
Continue iterating on this scheme until successive $h_j$ have converged.

Show that, for the present example, this algorithm converges to the optimal
policy function in one step.
